{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e1d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\anaconda3\\envs\\tfh1\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.0\n",
      "Hub version: 0.8.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#import tensorflow_addons as tfa\n",
    " \n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356684e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: efficientnetv2-s-21k : https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\n",
      "Input size (384, 384)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"efficientnetv2-s-21k\" \n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"swin_s3_base_224\": \"https://tfhub.dev/sayannath/mobilevit_s_1k_256/1\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"swin_s3_base_224\": 224, \n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821cdfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89514 files belonging to 33 classes.\n",
      "Using 71612 files for training.\n",
      "Found 89514 files belonging to 33 classes.\n",
      "Using 17902 files for validation.\n"
     ]
    }
   ],
   "source": [
    "data_dir = (\"./data/\")\n",
    "def build_dataset(subset):\n",
    "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=.20,\n",
    "      subset=subset,\n",
    "      label_mode=\"categorical\",\n",
    "      shuffle = True,\n",
    "      seed=1,\n",
    "      # Seed needs to provided when using validation_split and shuffle = True.\n",
    "      # A fixed seed is used so that the validation set is stable across runs.\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=1)\n",
    "\n",
    "train_ds = build_dataset(\"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "do_data_augmentation = True\n",
    "if do_data_augmentation:\n",
    "    preprocessing_model.add(\n",
    "        tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\"))\n",
    "\n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds = build_dataset(\"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f637028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 1280)              20331360  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 33)                42273     \n",
      "=================================================================\n",
      "Total params: 20,373,633\n",
      "Trainable params: 20,219,761\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "do_fine_tuning = True\n",
    "print(\"Building model with\", model_handle)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(len(class_names),\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24e1b5b-9bff-4cee-b93f-cda3826a94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1.567858893,\n",
    "                1: 1.002192296,\n",
    "                2: 1.002506266,\n",
    "                3: 1.000312598,\n",
    "                4: 1.33388912,\n",
    "                5: 1.480111008,\n",
    "                6: 1.335559265,\n",
    "                7: 1.333333333,\n",
    "                8: 1.000625391,\n",
    "                9: 1.486298189,\n",
    "                10: 1.013299557,\n",
    "                11: 1.000625391,\n",
    "                12: 1.33611691,\n",
    "                13: 1.33388912,\n",
    "                14: 1.33611691,\n",
    "                15: 1.334445371,\n",
    "                16: 1.33388912,\n",
    "                17: 1.006605851,\n",
    "                18: 1.505174036,\n",
    "                19: 1.436265709,\n",
    "                20: 1.013299557,\n",
    "                21: 1.372801373,\n",
    "                22: 1.333333333,\n",
    "                23: 1.335002086,\n",
    "                24: 1.000312598,\n",
    "                25: 1.427297056,\n",
    "                26: 1.336675021,\n",
    "                27: 1.000312598,\n",
    "                28: 1.000312598,\n",
    "                29: 1.000625391,\n",
    "                30: 1.001878522,\n",
    "                31: 1,\n",
    "                32: 1.001564945}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da03185",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_step = train_size // BATCH_SIZE\n",
    "lr_decayed_fn = (tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=0.03,t_mul=1,m_mul=1,first_decay_steps=f_step,alpha=0.0))\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9),\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7c1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5967/5967 [==============================] - 2518s 415ms/step - loss: 2.1077 - accuracy: 0.6261 - val_loss: 1.2952 - val_accuracy: 0.7934\n",
      "Epoch 2/10\n",
      "5967/5967 [==============================] - 2420s 405ms/step - loss: 1.7381 - accuracy: 0.7323 - val_loss: 1.2220 - val_accuracy: 0.8178\n",
      "Epoch 3/10\n",
      "5967/5967 [==============================] - 2415s 405ms/step - loss: 1.6144 - accuracy: 0.7678 - val_loss: 1.1920 - val_accuracy: 0.8293\n",
      "Epoch 4/10\n",
      "5967/5967 [==============================] - 2415s 405ms/step - loss: 1.5341 - accuracy: 0.7916 - val_loss: 1.1718 - val_accuracy: 0.8337\n",
      "Epoch 5/10\n",
      "5967/5967 [==============================] - 2414s 405ms/step - loss: 1.4737 - accuracy: 0.8071 - val_loss: 1.1537 - val_accuracy: 0.8371\n",
      "Epoch 6/10\n",
      "5967/5967 [==============================] - 2416s 405ms/step - loss: 1.4242 - accuracy: 0.8213 - val_loss: 1.1448 - val_accuracy: 0.8404\n",
      "Epoch 7/10\n",
      "5967/5967 [==============================] - 2416s 405ms/step - loss: 1.3801 - accuracy: 0.8333 - val_loss: 1.1415 - val_accuracy: 0.8398\n",
      "Epoch 8/10\n",
      "5967/5967 [==============================] - 2417s 405ms/step - loss: 1.3482 - accuracy: 0.8420 - val_loss: 1.1286 - val_accuracy: 0.8434\n",
      "Epoch 9/10\n",
      "5967/5967 [==============================] - 2415s 405ms/step - loss: 1.3178 - accuracy: 0.8494 - val_loss: 1.1306 - val_accuracy: 0.8426\n",
      "Epoch 10/10\n",
      "5967/5967 [==============================] - 2414s 405ms/step - loss: 1.2834 - accuracy: 0.8613 - val_loss: 1.1244 - val_accuracy: 0.8433\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = valid_size // BATCH_SIZE\n",
    "MyCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,restore_best_weights=True)\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[MyCallback],\n",
    "    class_weight=class_weight).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd267c5-a0e5-4116-8b42-52722344b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1555). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: v2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: v2s\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('v2s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eed3f88-130f-4d9b-a0b2-3ba44b597444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22308 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#model = keras.models.load_model('v2s')\n",
    "test_dir = (\"./final_test/\")\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    "    image_size=(384, 384),\n",
    "    shuffle=False,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "probability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])\n",
    "def process(image):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image\n",
    "test_ds = test_ds.map(process)\n",
    "predictions = probability_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7813326f-1b9f-4334-9299-309fb6edb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(22308)\n",
    "for i in range(0,22308):\n",
    "    a[i] = np.argmax(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9fee1e-402e-4d73-9d06-b85ea3979dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {0:'asparagus', 1:'bambooshoots', 2:'betel', 3:'broccoli', 4:'cauliflower', 5:'chinesecabbage', 6:'chinesechives', 7:'custardapple', 8:'grape', 9:'greenhouse', 10:'greenonion', 11:'kale', 12:'lemon', 13:'lettuce', 14:'litchi', 15:'longan', 16:'loofah', 17:'mango', 18:'onion', 19:'others', 20:'papaya', 21:'passionfruit', 22:'pear', 23:'pennisetum', 24:'redbeans', 25:'roseapple', 26:'sesbania', 27:'soybeans', 28:'sunhemp', 29:'sweetpotato', 30:'taro', 31:'tea', 32:'waterbamboo'}\n",
    "# Use the map() function to replace the values in the array\n",
    "b = np.array(list(map(lambda x: replacements.get(x, x), a)))\n",
    "np.savetxt('label.csv', b,fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26353f9-f191-48a2-8c94-61300dc262c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
